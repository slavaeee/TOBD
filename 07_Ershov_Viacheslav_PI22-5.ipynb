{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ершов Вячеслав ПИ22-5. Введение в обработку текста на естественном языке"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Материалы:\n",
    "* Макрушин С.В. Лекция 9: Введение в обработку текста на естественном языке\\\n",
    "* https://realpython.com/nltk-nlp-python/\n",
    "* https://scikit-learn.org/stable/modules/feature_extraction.html"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задачи для совместного разбора"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pymorphy2\n",
      "  Downloading pymorphy2-0.9.1-py3-none-any.whl (55 kB)\n",
      "Collecting docopt>=0.6\n",
      "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
      "Collecting dawg-python>=0.7.1\n",
      "  Downloading DAWG_Python-0.7.2-py2.py3-none-any.whl (11 kB)\n",
      "Collecting pymorphy2-dicts-ru<3.0,>=2.4\n",
      "  Downloading pymorphy2_dicts_ru-2.4.417127.4579844-py2.py3-none-any.whl (8.2 MB)\n",
      "Building wheels for collected packages: docopt\n",
      "  Building wheel for docopt (setup.py): started\n",
      "  Building wheel for docopt (setup.py): finished with status 'done'\n",
      "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13724 sha256=4fe37c313fcbbdf22323b7b56b7c1e25f81c6294d6eccceba6d6c20c9464b7f9\n",
      "  Stored in directory: c:\\users\\224995\\appdata\\local\\pip\\cache\\wheels\\70\\4a\\46\\1309fc853b8d395e60bafaf1b6df7845bdd82c95fd59dd8d2b\n",
      "Successfully built docopt\n",
      "Installing collected packages: pymorphy2-dicts-ru, docopt, dawg-python, pymorphy2\n",
      "Successfully installed dawg-python-0.7.2 docopt-0.6.2 pymorphy2-0.9.1 pymorphy2-dicts-ru-2.4.417127.4579844\n",
      "Collecting Levenshtein\n",
      "  Downloading Levenshtein-0.21.0-cp39-cp39-win_amd64.whl (101 kB)\n",
      "Collecting rapidfuzz<4.0.0,>=2.3.0\n",
      "  Downloading rapidfuzz-3.0.0-cp39-cp39-win_amd64.whl (1.8 MB)\n",
      "Installing collected packages: rapidfuzz, Levenshtein\n",
      "Successfully installed Levenshtein-0.21.0 rapidfuzz-3.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install pymorphy2\n",
    "!pip install Levenshtein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pymorphy2\n",
    "import nltk\n",
    "from nltk import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "import Levenshtein"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Считайте слова из файла `litw-win.txt` и запишите их в список `words`. В заданном предложении исправьте все опечатки, заменив слова с опечатками на ближайшие (в смысле расстояния Левенштейна) к ним слова из списка `words`. Считайте, что в слове есть опечатка, если данное слово не содержится в списке `words`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "с велечайшим усилием выбравшись из потока убегающих людей Кутузов с свитой уменьшевшейся вдвое поехал на звуки выстрелов русских орудий\n"
     ]
    }
   ],
   "source": [
    "with open('litw-win.txt', encoding='utf-8') as f:\n",
    "    words = f.read().split()\n",
    "sentence = \"С велечайшем усилен выбравшись из потоко убыгаюших людей, Кутузов са светой, уменшившийса вдвое, пояхал на звуки выстрелов руских орудий.\"\n",
    "new_sentence = []\n",
    "for word in sentence.split():\n",
    "    if word in words:\n",
    "        new_sentence.append(word)\n",
    "    else:\n",
    "        distances = [(editdistance.eval(word, w), w) for w in words]\n",
    "        closest_word = min(distances, key=lambda x: x[0])[1]\n",
    "        new_sentence.append(closest_word)\n",
    "new_sentence = ' '.join(new_sentence)\n",
    "print(new_sentence)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Разбейте текст из формулировки задания 1 на слова; проведите стемминг и лемматизацию слов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "слова: ['Считайте', 'слова', 'из', 'файла', 'litw-win.txt', 'и', 'запишите', 'их', 'в', 'список', 'words', '.', 'В', 'заданном', 'предложении', 'исправьте', 'все', 'опечатки', ',', 'заменив', 'слова', 'с', 'опечатками', 'на', 'ближайшие', '(', 'в', 'смысле', 'расстояния', 'Левенштейна', ')', 'к', 'ним', 'слова', 'из', 'списка', 'words', '.', 'Считайте', ',', 'что', 'в', 'слове', 'есть', 'опечатка', ',', 'если', 'данное', 'слово', 'не', 'содержится', 'в', 'списке', 'words', '.']\n",
      "\n",
      "стемы: ['счита', 'слов', 'из', 'файл', 'litw-win.txt', 'и', 'запиш', 'их', 'в', 'список', 'words', '.', 'в', 'зада', 'предложен', 'исправьт', 'все', 'опечатк', ',', 'замен', 'слов', 'с', 'опечатк', 'на', 'ближайш', '(', 'в', 'смысл', 'расстоян', 'левенштейн', ')', 'к', 'ним', 'слов', 'из', 'списк', 'words', '.', 'счита', ',', 'что', 'в', 'слов', 'ест', 'опечатк', ',', 'есл', 'дан', 'слов', 'не', 'содерж', 'в', 'списк', 'words', '.']\n",
      "\n",
      "леммы: ['Считайте', 'слова', 'из', 'файла', 'litw-win.txt', 'и', 'запишите', 'их', 'в', 'список', 'word', '.', 'В', 'заданном', 'предложении', 'исправьте', 'все', 'опечатки', ',', 'заменив', 'слова', 'с', 'опечатками', 'на', 'ближайшие', '(', 'в', 'смысле', 'расстояния', 'Левенштейна', ')', 'к', 'ним', 'слова', 'из', 'списка', 'word', '.', 'Считайте', ',', 'что', 'в', 'слове', 'есть', 'опечатка', ',', 'если', 'данное', 'слово', 'не', 'содержится', 'в', 'списке', 'word', '.']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import SnowballStemmer, WordNetLemmatizer\n",
    "text = 'Считайте слова из файла litw-win.txt и запишите их в список words. В заданном предложении исправьте все опечатки, заменив слова с опечатками на ближайшие (в смысле расстояния Левенштейна) к ним слова из списка words. Считайте, что в слове есть опечатка, если данное слово не содержится в списке words.'\n",
    "words = nltk.word_tokenize(text)\n",
    "stemmer = SnowballStemmer('russian')\n",
    "stems = [stemmer.stem(word) for word in words]\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmas = [lemmatizer.lemmatize(word) for word in words]\n",
    "print(f'слова: {words}\\n')\n",
    "print(f'стемы: {stems}\\n')\n",
    "print(f'леммы: {lemmas}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Преобразуйте предложения из формулировки задания 1 в векторы при помощи `CountVectorizer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['litw', 'txt', 'win', 'words', 'ближайшие', 'все', 'данное', 'если', 'есть', 'заданном', 'заменив', 'запишите', 'из', 'исправьте', 'их', 'левенштейна', 'на', 'не', 'ним', 'опечатка', 'опечатками', 'опечатки', 'предложении', 'расстояния', 'слова', 'слове', 'слово', 'смысле', 'содержится', 'списка', 'списке', 'список', 'считайте', 'файла', 'что']\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'numpy.ndarray'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_14580/3385283670.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mwords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_feature_names\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mvectorized_text\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: unhashable type: 'numpy.ndarray'"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "text = 'Считайте слова из файла litw-win.txt и запишите их в список words. В заданном предложении исправьте все опечатки, заменив слова с опечатками на ближайшие (в смысле расстояния Левенштейна) к ним слова из списка words. Считайте, что в слове есть опечатка, если данное слово не содержится в списке words.'\n",
    "vectorizer = CountVectorizer()\n",
    "vectorized_text = vectorizer.fit_transform([text])\n",
    "words = vectorizer.get_feature_names()\n",
    "print(words)\n",
    "print({vectorized_text.toarray()})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Лабораторная работа 9"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Расстояние редактирования"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.1 Загрузите предобработанные описания рецептов из файла `preprocessed_descriptions.csv`. Получите набор уникальных слов `words`, содержащихся в текстах описаний рецептов (воспользуйтесь `word_tokenize` из `nltk`). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.stem import SnowballStemmer, WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "argument of type 'LazyCorpusLoader' is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_14580/4128653207.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mprocessed_text\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[0mrecipes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'description'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrecipes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'description'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpreprocess_text\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[0mrecipes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'preprocessed_descriptions.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[0;32m   4355\u001b[0m         \u001b[0mdtype\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mfloat64\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4356\u001b[0m         \"\"\"\n\u001b[1;32m-> 4357\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mSeriesApply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconvert_dtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4358\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4359\u001b[0m     def _reduce(\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\apply.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1041\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_str\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1042\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1043\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_standard\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1044\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1045\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0magg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\apply.py\u001b[0m in \u001b[0;36mapply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1096\u001b[0m                 \u001b[1;31m# List[Union[Callable[..., Any], str]]]]]\"; expected\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1097\u001b[0m                 \u001b[1;31m# \"Callable[[Any], Any]\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1098\u001b[1;33m                 mapped = lib.map_infer(\n\u001b[0m\u001b[0;32m   1099\u001b[0m                     \u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1100\u001b[0m                     \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# type: ignore[arg-type]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\_libs\\lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_14580/4128653207.py\u001b[0m in \u001b[0;36mpreprocess_text\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mpreprocess_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mtokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0mfiltered_tokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtoken\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtokens\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m     \u001b[0mlemmatized_tokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mlemmatizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfiltered_tokens\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mprocessed_text\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m' '\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlemmatized_tokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_14580/4128653207.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mpreprocess_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mtokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0mfiltered_tokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtoken\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtokens\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m     \u001b[0mlemmatized_tokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mlemmatizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfiltered_tokens\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mprocessed_text\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m' '\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlemmatized_tokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: argument of type 'LazyCorpusLoader' is not iterable"
     ]
    }
   ],
   "source": [
    "recipes = pd.read_csv('recipes_sample.csv')\n",
    "tokenizer = nltk.RegexpTokenizer(r'\\w+')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def preprocess_text(text):\n",
    "    tokens = tokenizer.tokenize(str(text))\n",
    "    filtered_tokens = [token for token in tokens if token not in stopwords]\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in filtered_tokens]\n",
    "    processed_text = ' '.join(lemmatized_tokens)\n",
    "    return processed_text\n",
    "recipes['description'] = recipes['description'].apply(preprocess_text)\n",
    "recipes.to_csv('preprocessed_descriptions.csv', index=False)\n",
    "df = pd.read_csv('preprocessed_descriptions.csv')\n",
    "description_list = df['description'].dropna().tolist() # исключаем пропущенные значения\n",
    "words = set(word_tokenize(' '.join(description_list)))\n",
    "print(list(words)[:50])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.2 Сгенерируйте 5 пар случайно выбранных слов и посчитайте между ними расстояние редактирования."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mgm reading 7\n",
      "orlando distracted 9\n",
      "deepfry covenient 8\n",
      "soaked cover 4\n",
      "wether supplementing 11\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from Levenshtein import distance\n",
    "for i in range(5):\n",
    "    word1, word2 = random.sample(list(words), 2)\n",
    "    dist = distance(word1, word2)\n",
    "    print(word1, word2, dist)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.3 Напишите функцию, которая для заданного слова `word` возвращает `k` ближайших к нему слов из списка `words` (близость слов измеряется с помощью расстояния Левенштейна)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'digest': 6,\n",
       " 'denti': 4,\n",
       " 'spinning': 7,\n",
       " 'volleball': 8,\n",
       " 'hockett': 7,\n",
       " 'leader': 5,\n",
       " '56459': 5,\n",
       " 'reinhard': 7,\n",
       " 'seasame': 6,\n",
       " 'delhi': 5,\n",
       " 'armenia': 5,\n",
       " 'damp': 3,\n",
       " 'feisty': 6,\n",
       " 'complexity': 9,\n",
       " 'ndtv': 4,\n",
       " 'penne': 4,\n",
       " 'duration': 6,\n",
       " 'farther': 6,\n",
       " 'boca': 4,\n",
       " 'benefiber': 8,\n",
       " 'grandpa': 5,\n",
       " 'attain': 4,\n",
       " 'forgiving': 8,\n",
       " 'burnt': 4,\n",
       " 'dil': 3,\n",
       " 'erik': 4,\n",
       " '10cm': 4,\n",
       " 'corpse': 6,\n",
       " 'gourd': 5,\n",
       " 'stiller': 7,\n",
       " 'brigitte': 8,\n",
       " 'advantage': 7,\n",
       " 'annato': 5,\n",
       " 'stiffing': 7,\n",
       " 'ribeye': 6,\n",
       " 'betacarotin': 9,\n",
       " 'partridge': 8,\n",
       " 'manages': 4,\n",
       " 'pennsylvania': 10,\n",
       " 'orginated': 8,\n",
       " 'daunting': 6,\n",
       " 'kho': 3,\n",
       " '193370': 6,\n",
       " 'assemble': 7,\n",
       " 'phtml': 5,\n",
       " 'doomed': 5,\n",
       " 'knack': 4,\n",
       " 'subsitute': 9,\n",
       " 'tumlin': 4,\n",
       " 'neely': 5,\n",
       " 'jose': 4,\n",
       " 'underdried': 10,\n",
       " 'talk': 3,\n",
       " 'smidgen': 5,\n",
       " 'sauté': 4,\n",
       " 'alternatively': 12,\n",
       " 'binuya': 5,\n",
       " 'decor': 5,\n",
       " 'tout': 4,\n",
       " '222678': 6,\n",
       " 'fam': 2,\n",
       " 'harvest': 6,\n",
       " 'buttery': 7,\n",
       " 'wanting': 5,\n",
       " 'redolent': 7,\n",
       " 'c2009': 5,\n",
       " 'benifits': 7,\n",
       " '45672': 5,\n",
       " 'butterflied': 11,\n",
       " 'gasllon': 5,\n",
       " 'marmite': 5,\n",
       " 'tucked': 6,\n",
       " 'peshawar': 7,\n",
       " 'phulkas': 6,\n",
       " 'unrinary': 7,\n",
       " 'legume': 6,\n",
       " '200c': 4,\n",
       " 'dry': 3,\n",
       " 'deridder': 8,\n",
       " 'fiqures': 7,\n",
       " 'discus': 6,\n",
       " 'wolfgang': 6,\n",
       " 'lane': 2,\n",
       " 'demitasse': 7,\n",
       " 'johnson': 6,\n",
       " 'desciption': 9,\n",
       " 'cholent': 6,\n",
       " 'ellegant': 6,\n",
       " 'moroccan': 5,\n",
       " 'adulterant': 8,\n",
       " 'garnish': 5,\n",
       " 'sunset': 5,\n",
       " 'contestant': 8,\n",
       " 'along': 4,\n",
       " 'marchena': 5,\n",
       " 'wakame': 5,\n",
       " 'pradesh': 6,\n",
       " 'gadget': 5,\n",
       " '101723': 6,\n",
       " 'silpat': 5,\n",
       " 'nowhere': 7,\n",
       " 'yemarina': 5,\n",
       " 'found': 4,\n",
       " 'sanitarium': 8,\n",
       " 'chemotherapy': 10,\n",
       " 'submission': 8,\n",
       " 'carlucci': 7,\n",
       " 'dh': 3,\n",
       " 'unless': 6,\n",
       " 'apron': 4,\n",
       " 'sweetheart': 9,\n",
       " '187': 3,\n",
       " '84': 3,\n",
       " 'standing': 6,\n",
       " 'barbari': 6,\n",
       " 'panama': 4,\n",
       " 'basalmic': 7,\n",
       " 'tapper': 5,\n",
       " 'stated': 5,\n",
       " 'fine': 3,\n",
       " '2lbs': 4,\n",
       " 'winging': 6,\n",
       " 'concord': 6,\n",
       " 'hussien': 6,\n",
       " 'pitchfork': 9,\n",
       " 'wholly': 6,\n",
       " 'hav': 2,\n",
       " 'radical': 6,\n",
       " 'lonavala': 7,\n",
       " 'welcomed': 7,\n",
       " 'fill': 4,\n",
       " 'bedroom': 7,\n",
       " 'astrid': 6,\n",
       " 'growing': 6,\n",
       " 'flavone': 5,\n",
       " 'generously': 9,\n",
       " 'tangy': 3,\n",
       " 'method': 5,\n",
       " 'eastenders': 8,\n",
       " 'clayton': 5,\n",
       " 'roni': 3,\n",
       " 'screw': 5,\n",
       " 'reisman': 4,\n",
       " 'label': 4,\n",
       " 'swears': 5,\n",
       " 'tasteitalia': 10,\n",
       " 'compatible': 8,\n",
       " 'riina': 4,\n",
       " 'swig': 4,\n",
       " 'ashton': 5,\n",
       " 'fiancé': 4,\n",
       " 'spartanburg': 9,\n",
       " 'disneyfamily': 11,\n",
       " 're': 3,\n",
       " 'rich': 4,\n",
       " 'posh': 4,\n",
       " 'ellie': 5,\n",
       " 'hundred': 6,\n",
       " '820': 3,\n",
       " 'vicksburg': 9,\n",
       " 'creepy': 6,\n",
       " 'ballet': 5,\n",
       " 'consuming': 7,\n",
       " 'quirky': 6,\n",
       " 'hubby': 5,\n",
       " 'although': 8,\n",
       " 'tim': 3,\n",
       " 'colophon': 7,\n",
       " 'stevens': 6,\n",
       " 'perpetual': 8,\n",
       " 'boule': 5,\n",
       " 'handful': 5,\n",
       " 'chum': 4,\n",
       " 'bulgarian': 7,\n",
       " 'markon': 3,\n",
       " 'häme': 4,\n",
       " 'magnificence': 9,\n",
       " 'produce': 7,\n",
       " 'steavens': 6,\n",
       " 'dentist': 6,\n",
       " '89007': 5,\n",
       " 'dp': 3,\n",
       " 'vivian': 4,\n",
       " 'parkerhouse': 10,\n",
       " 'amysauntjo': 7,\n",
       " 'shrove': 6,\n",
       " 'plait': 4,\n",
       " 'cassrole': 7,\n",
       " 'knife': 5,\n",
       " 'hot': 3,\n",
       " 'marinade': 5,\n",
       " 'bannock': 5,\n",
       " 'hotdishes': 9,\n",
       " 'apr': 3,\n",
       " 'fletcher': 8,\n",
       " 'moroccanfood': 9,\n",
       " 'refered': 7,\n",
       " 'individual': 9,\n",
       " 'mimeographed': 10,\n",
       " 'perricone': 8,\n",
       " 'toward': 5,\n",
       " 'wholegrain': 8,\n",
       " 'hoppin': 5,\n",
       " 'rda': 3,\n",
       " 'varying': 5,\n",
       " 'many': 1,\n",
       " 'ichimi': 6,\n",
       " 'sommerfeste': 10,\n",
       " 'chengdu': 6,\n",
       " 'momo': 3,\n",
       " 'rewarded': 7,\n",
       " 'aerosol': 7,\n",
       " 'bibb': 4,\n",
       " 'calender': 6,\n",
       " 'schmy': 5,\n",
       " 'borekas': 6,\n",
       " 'eberswalder': 10,\n",
       " 'aschkuchen': 9,\n",
       " 'laboring': 6,\n",
       " 'örebro': 6,\n",
       " 'heresy': 6,\n",
       " 'fails': 4,\n",
       " 'wildernessfamilynaturals': 22,\n",
       " 'knuckle': 7,\n",
       " 'fitness': 6,\n",
       " 'elder': 5,\n",
       " 'roiling': 6,\n",
       " 'dreamy': 5,\n",
       " 'nope': 4,\n",
       " 'enought': 7,\n",
       " 'strapped': 7,\n",
       " 'ounce': 4,\n",
       " 'loooking': 7,\n",
       " 'thermometer': 10,\n",
       " 'aspx': 4,\n",
       " 'mafalde': 5,\n",
       " 'adjusted': 8,\n",
       " '95mg': 4,\n",
       " 'seicana': 5,\n",
       " 'nd': 3,\n",
       " 'drug': 4,\n",
       " 'fairfield': 8,\n",
       " '454028': 6,\n",
       " '389128': 6,\n",
       " 'experimenting': 11,\n",
       " 'grove': 5,\n",
       " 'indentation': 9,\n",
       " 'cave': 3,\n",
       " 'schatz': 5,\n",
       " 'robinson': 7,\n",
       " 'fitzer': 6,\n",
       " 'carbondale': 8,\n",
       " 'large': 4,\n",
       " 'sprung': 5,\n",
       " 'grabbing': 6,\n",
       " 'wt8': 3,\n",
       " 'binnur': 5,\n",
       " 'dinnertime': 9,\n",
       " 'challenged': 8,\n",
       " 'lure': 4,\n",
       " 'mediterrasian': 10,\n",
       " 'granddaughter': 11,\n",
       " 'latina': 4,\n",
       " 'calculating': 9,\n",
       " 'watcher': 6,\n",
       " 'actor': 5,\n",
       " 'phosphorus': 10,\n",
       " 'pavilion': 6,\n",
       " 'cloud': 5,\n",
       " 'forever': 7,\n",
       " 'guittard': 7,\n",
       " 'lemonade': 6,\n",
       " 'disk': 4,\n",
       " 'livens': 5,\n",
       " 'alternate': 8,\n",
       " 'margaret': 6,\n",
       " 'choker': 6,\n",
       " 'oversized': 9,\n",
       " 'cowberry': 8,\n",
       " 'mambo': 3,\n",
       " 'portuguese': 10,\n",
       " 'enormous': 7,\n",
       " 'consummate': 8,\n",
       " '1938': 4,\n",
       " 'praise': 5,\n",
       " 'bulk': 4,\n",
       " 'hootie': 6,\n",
       " '9x5': 3,\n",
       " '400mg': 5,\n",
       " 'nabisco': 6,\n",
       " 'waking': 4,\n",
       " 'cactus': 5,\n",
       " 'unroll': 6,\n",
       " 'hollye': 6,\n",
       " 'ave': 3,\n",
       " 'katy': 3,\n",
       " 'formal': 4,\n",
       " 'simulate': 6,\n",
       " 'tassajara': 8,\n",
       " 'chhicken': 7,\n",
       " 'harira': 5,\n",
       " 'housed': 6,\n",
       " 'committee': 8,\n",
       " 'counterpoint': 11,\n",
       " 'ee': 3,\n",
       " 'formed': 5,\n",
       " 'manna': 2,\n",
       " 'lonewolf': 7,\n",
       " 'weaker': 5,\n",
       " 'pastic': 5,\n",
       " 'forty': 5,\n",
       " 'intestinal': 9,\n",
       " 'ooooooooozing': 12,\n",
       " '2006': 4,\n",
       " 'garnished': 7,\n",
       " 'rely': 4,\n",
       " 'lora': 4,\n",
       " 'library': 6,\n",
       " 'vista': 5,\n",
       " 'mignons': 5,\n",
       " 'challenging': 9,\n",
       " 'sharing': 5,\n",
       " 'extremley': 8,\n",
       " '59mg': 4,\n",
       " 'refridgerated': 12,\n",
       " 'nummmmy': 6,\n",
       " 'glas': 3,\n",
       " 'orphan': 4,\n",
       " 'piggy': 5,\n",
       " 'spinach': 6,\n",
       " 'sandi': 3,\n",
       " 'aye': 3,\n",
       " 'blanchard': 7,\n",
       " 'loveable': 7,\n",
       " 'reviewer': 8,\n",
       " 'enjoys': 6,\n",
       " 'roll': 4,\n",
       " 'old': 3,\n",
       " 'grazing': 5,\n",
       " 'pencil': 5,\n",
       " '100': 3,\n",
       " 'creamy': 5,\n",
       " 'agulas': 5,\n",
       " 'santee': 4,\n",
       " 'hangover': 6,\n",
       " 'crust': 5,\n",
       " 'adai': 3,\n",
       " 'dashi': 4,\n",
       " 'drummond': 6,\n",
       " 'tebben': 5,\n",
       " 'pure': 4,\n",
       " '118894': 6,\n",
       " 'entertaining': 10,\n",
       " 'beignet': 6,\n",
       " 'origianal': 7,\n",
       " 'manju': 2,\n",
       " 'saskatchewan': 10,\n",
       " 'elevation': 7,\n",
       " '60314': 5,\n",
       " 'bach': 3,\n",
       " 'kelly': 5,\n",
       " 'joan': 2,\n",
       " 'safe': 3,\n",
       " 'uncovered': 9,\n",
       " 'gran': 2,\n",
       " 'poly': 4,\n",
       " '61262': 5,\n",
       " 'mezze': 4,\n",
       " 'subtler': 7,\n",
       " 'wa': 2,\n",
       " 'bowersox': 8,\n",
       " 'hbo': 3,\n",
       " 'brought': 7,\n",
       " 'weinstein': 8,\n",
       " 'crteative': 8,\n",
       " 'petit': 5,\n",
       " 'pad': 2,\n",
       " 'marcella': 6,\n",
       " 'himalayan': 6,\n",
       " 'venus': 4,\n",
       " 'permanent': 6,\n",
       " 'bobby': 5,\n",
       " 'fruitier': 8,\n",
       " 'ramsey': 5,\n",
       " 'sherrie': 7,\n",
       " 'sid': 3,\n",
       " 'colore': 6,\n",
       " 'focused': 7,\n",
       " 'bley': 4,\n",
       " 'ode': 3,\n",
       " 'reactive': 7,\n",
       " 'lhj': 3,\n",
       " 'smile': 4,\n",
       " 'manx': 1,\n",
       " 'gidiet': 6,\n",
       " 'simiple': 6,\n",
       " 'dose': 4,\n",
       " 'transmitted': 9,\n",
       " 'created': 6,\n",
       " 'hogie': 5,\n",
       " 'seviche': 7,\n",
       " 'samp': 3,\n",
       " 'unpleasant': 8,\n",
       " 'fudgie': 6,\n",
       " 'anshan': 4,\n",
       " 'honorary': 7,\n",
       " 'ct': 3,\n",
       " 'jaques': 5,\n",
       " 'reception': 8,\n",
       " 'want': 2,\n",
       " 'learnt': 4,\n",
       " 'fizzano': 5,\n",
       " 'potted': 6,\n",
       " 'anything': 7,\n",
       " 'emphasizing': 8,\n",
       " 'workshop': 8,\n",
       " 'shrunk': 5,\n",
       " 'depending': 8,\n",
       " 'enamel': 5,\n",
       " 'perth': 5,\n",
       " 'ronnen': 5,\n",
       " 'haddock': 6,\n",
       " 'shchi': 5,\n",
       " 'scot': 4,\n",
       " 'frequent': 7,\n",
       " 'yoghurt': 7,\n",
       " 'mozart': 4,\n",
       " 'laundromat': 8,\n",
       " 'oaky': 3,\n",
       " 'brightly': 8,\n",
       " 'leaden': 4,\n",
       " 'office': 6,\n",
       " 'annapolis': 8,\n",
       " 'whim': 4,\n",
       " 'cutest': 6,\n",
       " 'burkhard': 7,\n",
       " 'overlap': 6,\n",
       " 'iraq': 3,\n",
       " 'explode': 7,\n",
       " 'ghanouj': 5,\n",
       " 'trove': 5,\n",
       " 'extraordinary': 11,\n",
       " 'rd': 3,\n",
       " 'rubel': 5,\n",
       " 'yay': 2,\n",
       " 'croustade': 8,\n",
       " 'choir': 5,\n",
       " '158': 3,\n",
       " 'tuned': 4,\n",
       " '455142': 6,\n",
       " 'saucing': 5,\n",
       " 'absoloutly': 10,\n",
       " 'mastering': 6,\n",
       " 'disappearing': 10,\n",
       " 'drizzling': 8,\n",
       " 'clientel': 7,\n",
       " 'broihier': 8,\n",
       " 'drab': 3,\n",
       " 'alston': 5,\n",
       " 'tedious': 7,\n",
       " '13g': 3,\n",
       " 'whilwind': 7,\n",
       " 'declining': 8,\n",
       " 'work': 4,\n",
       " 'fixing': 5,\n",
       " 'chipolte': 8,\n",
       " 'customary': 7,\n",
       " 'filo': 4,\n",
       " 'dion': 3,\n",
       " 'lucile': 6,\n",
       " 'mounded': 5,\n",
       " 'ste': 3,\n",
       " 'insane': 4,\n",
       " 'dilly': 5,\n",
       " 'practise': 7,\n",
       " 'shanghai': 6,\n",
       " 'rapid': 4,\n",
       " 'shakey': 5,\n",
       " 'bassett': 6,\n",
       " 'deepen': 5,\n",
       " 'loving': 5,\n",
       " 'q': 3,\n",
       " 'ghah': 3,\n",
       " 'testing': 6,\n",
       " 'staring': 5,\n",
       " 'powderinstead': 12,\n",
       " 'purest': 6,\n",
       " 'plateful': 7,\n",
       " 'weep': 4,\n",
       " 'ring': 3,\n",
       " 'crystalized': 10,\n",
       " 'wrinkle': 6,\n",
       " 'donut': 4,\n",
       " 'serendipity': 10,\n",
       " 'docteur': 7,\n",
       " 'utility': 7,\n",
       " 'fairmont': 6,\n",
       " 'believed': 8,\n",
       " 'pureed': 6,\n",
       " 'ethyl': 5,\n",
       " 'oven': 3,\n",
       " 'pepperidge': 10,\n",
       " 'surround': 7,\n",
       " 'crowd': 5,\n",
       " 'ccoking': 6,\n",
       " 'think': 4,\n",
       " 'tail': 3,\n",
       " 'jolly': 5,\n",
       " 'fillingly': 8,\n",
       " 'janis': 3,\n",
       " 'patuleia': 7,\n",
       " 'symbolize': 8,\n",
       " 'subterfuge': 10,\n",
       " 'amerindian': 7,\n",
       " 'vegeterian': 8,\n",
       " 'moistest': 7,\n",
       " 'shelburne': 8,\n",
       " 'button': 5,\n",
       " 'dealer': 5,\n",
       " 'swordfish': 9,\n",
       " 'arabia': 5,\n",
       " 'bewitched': 9,\n",
       " 'hawaiinrecipes': 12,\n",
       " 'taverna': 5,\n",
       " 'flavoring': 7,\n",
       " 'remodeling': 8,\n",
       " 'nat': 2,\n",
       " 'banger': 4,\n",
       " 'warm': 3,\n",
       " 'revived': 7,\n",
       " 'traveled': 7,\n",
       " 'cable': 4,\n",
       " 'dishid': 6,\n",
       " 'moong': 3,\n",
       " 'bad': 2,\n",
       " 'entirely': 8,\n",
       " 'ottolenghi': 9,\n",
       " 'eventually': 9,\n",
       " 'confite': 6,\n",
       " 'fiance': 4,\n",
       " 'quasi': 4,\n",
       " 'immune': 4,\n",
       " 'applewood': 9,\n",
       " 'b12': 3,\n",
       " 'banff': 3,\n",
       " 'known': 4,\n",
       " 'parathas': 7,\n",
       " 'laughton': 6,\n",
       " 'hg': 3,\n",
       " 'huynh': 4,\n",
       " 'offset': 6,\n",
       " '211970': 6,\n",
       " 'bettina': 6,\n",
       " 'booster': 7,\n",
       " 'nikibone': 7,\n",
       " 'grew': 4,\n",
       " 'enveloped': 9,\n",
       " 'brar': 3,\n",
       " 'tokyo': 5,\n",
       " 'chocoholic': 10,\n",
       " 'teir': 4,\n",
       " 'yam': 2,\n",
       " 'pureeing': 7,\n",
       " 'gouda': 5,\n",
       " '150466': 6,\n",
       " 'clem': 4,\n",
       " 'catherine': 7,\n",
       " 'homeland': 5,\n",
       " 'ladybug': 6,\n",
       " 'gulyas': 5,\n",
       " 'editor': 6,\n",
       " 'hillshire': 9,\n",
       " 'batch': 4,\n",
       " '51104': 5,\n",
       " 'portabellos': 10,\n",
       " 'strengthen': 9,\n",
       " 'mocktails': 7,\n",
       " 'entenmann': 6,\n",
       " 'quincy': 5,\n",
       " 'verdes': 6,\n",
       " 'hometown': 6,\n",
       " 'bronchial': 8,\n",
       " 'beggar': 5,\n",
       " 'barossa': 6,\n",
       " '001': 3,\n",
       " 'krautheimer': 10,\n",
       " 'walton': 4,\n",
       " 'spell': 5,\n",
       " 'jo': 3,\n",
       " 'eremich': 6,\n",
       " 'recipezaar': 9,\n",
       " 'frappacchino': 10,\n",
       " 'fiver': 5,\n",
       " 'gascon': 4,\n",
       " 'cbw': 3,\n",
       " 'presented': 8,\n",
       " 'teaspoonful': 9,\n",
       " 'destroy': 7,\n",
       " 'win': 2,\n",
       " 'saag': 3,\n",
       " 'veges': 5,\n",
       " 'spectacular': 10,\n",
       " 'blackmoons': 8,\n",
       " 'poppy': 5,\n",
       " 'karlsen': 5,\n",
       " 'cleanse': 5,\n",
       " 'rigate': 5,\n",
       " '309199': 6,\n",
       " 'guerrero': 8,\n",
       " 'cauldron': 6,\n",
       " 'ribbon': 5,\n",
       " 'league': 5,\n",
       " 'educational': 9,\n",
       " 'lap': 2,\n",
       " 'vermentino': 8,\n",
       " 'curve': 5,\n",
       " 'rugelach': 7,\n",
       " 'buffalo': 6,\n",
       " 'recently': 7,\n",
       " 'bearnaise': 7,\n",
       " 'invention': 8,\n",
       " 'orginal': 6,\n",
       " '146': 3,\n",
       " 'endearing': 7,\n",
       " 'koodos': 6,\n",
       " 'enlightment': 9,\n",
       " 'female': 4,\n",
       " 'fabulour': 7,\n",
       " 'reconsituting': 12,\n",
       " 'portioned': 8,\n",
       " 'mypersiankitchen': 13,\n",
       " 'smash': 3,\n",
       " 'furikake': 7,\n",
       " 'mace': 2,\n",
       " 'strifry': 7,\n",
       " 'indimidate': 8,\n",
       " 'nigeria': 7,\n",
       " 'came': 3,\n",
       " 'rc': 3,\n",
       " 'marketed': 6,\n",
       " 'haunt': 3,\n",
       " 'alzheimers': 9,\n",
       " 'forgo': 5,\n",
       " 'tappa': 4,\n",
       " 'dab': 2,\n",
       " '30g': 3,\n",
       " 'salon': 3,\n",
       " 'breadstick': 9,\n",
       " '20mls': 4,\n",
       " 'ciudad': 5,\n",
       " 'boosted': 7,\n",
       " 'whole9life': 10,\n",
       " 'carawasy': 7,\n",
       " 'bahrain': 5,\n",
       " 'dont': 3,\n",
       " 'calavo': 5,\n",
       " 'boozy': 5,\n",
       " 'neighbor': 8,\n",
       " 'cultural': 7,\n",
       " 'yk': 3,\n",
       " 'grit': 4,\n",
       " 'chilli': 6,\n",
       " 'collaboration': 11,\n",
       " 'nouveau': 6,\n",
       " 'sale': 3,\n",
       " 'eared': 4,\n",
       " '182': 3,\n",
       " 'feat': 3,\n",
       " 'bistro': 6,\n",
       " 'juan': 2,\n",
       " 'appearance': 8,\n",
       " 'wopping': 6,\n",
       " '80': 3,\n",
       " 'broome': 6,\n",
       " 'overwhelms': 10,\n",
       " 'texas': 4,\n",
       " 'ilovepickles': 12,\n",
       " 'mmmmm': 4,\n",
       " 'akin': 3,\n",
       " 'customize': 8,\n",
       " 'residence': 8,\n",
       " 'czechoslovak': 11,\n",
       " 'mitchell': 7,\n",
       " 'cordero': 7,\n",
       " 'em': 3,\n",
       " 'management': 7,\n",
       " 'paleomg': 6,\n",
       " 'browner': 6,\n",
       " 'dynamite': 7,\n",
       " 'suhanosky': 7,\n",
       " 'adventurous': 10,\n",
       " 'saucier': 6,\n",
       " 'storage': 6,\n",
       " 'pecorino': 7,\n",
       " 'clotilde': 8,\n",
       " 'applied': 7,\n",
       " '15mos': 4,\n",
       " 'sarnie': 4,\n",
       " 'marinaded': 6,\n",
       " 'guinness': 7,\n",
       " 'bernhardt': 8,\n",
       " 'belinda': 6,\n",
       " 'chocolatecakes': 13,\n",
       " 'coveted': 7,\n",
       " 'tackle': 5,\n",
       " 'barcardi': 7,\n",
       " 'bethel': 6,\n",
       " 'inspired': 8,\n",
       " 'middletown': 8,\n",
       " 'liberate': 7,\n",
       " 'julienne': 7,\n",
       " 'heatherfeather': 13,\n",
       " 'gastronomy': 8,\n",
       " 'slimming': 6,\n",
       " 'strach': 5,\n",
       " 'lighter': 7,\n",
       " 'rowed': 5,\n",
       " 'scoop': 5,\n",
       " 'swedish': 7,\n",
       " 'civilization': 10,\n",
       " 'flatten': 5,\n",
       " 'fêtes': 5,\n",
       " 'sherman': 4,\n",
       " 'carole': 5,\n",
       " 'marseille': 7,\n",
       " 'dianne': 4,\n",
       " 'rural': 4,\n",
       " 'juncalito': 8,\n",
       " 'beard': 4,\n",
       " 'gee': 3,\n",
       " 'brooklyn': 7,\n",
       " 'plot': 4,\n",
       " 'katie': 4,\n",
       " 'opposite': 8,\n",
       " 'itallian': 6,\n",
       " 'bird': 4,\n",
       " 'angostura': 8,\n",
       " 'alot': 4,\n",
       " 'marketing': 6,\n",
       " 'quadra': 5,\n",
       " 'intensly': 7,\n",
       " 'appetizer': 9,\n",
       " 'caramely': 7,\n",
       " 'lucinda': 6,\n",
       " 'grey': 4,\n",
       " 'souffle': 7,\n",
       " 'malbec': 4,\n",
       " 'caribbean': 7,\n",
       " 'hinged': 5,\n",
       " 'schloss': 7,\n",
       " 'rouge': 5,\n",
       " 'stiltson': 7,\n",
       " 'tape': 3,\n",
       " 'mincing': 5,\n",
       " 'business': 7,\n",
       " '140055': 6,\n",
       " 'certifiedangusbeef': 16,\n",
       " 'zimmern': 5,\n",
       " 'scented': 6,\n",
       " 'makeawishutah': 11,\n",
       " '24k': 3,\n",
       " 'boysenberry': 10,\n",
       " 'waverman': 5,\n",
       " 'charles': 6,\n",
       " 'svenskarnas': 9,\n",
       " 'pleasant': 6,\n",
       " 'noise': 5,\n",
       " 'marinading': 7,\n",
       " '23840': 5,\n",
       " 'contrasting': 9,\n",
       " 'browneyedbaker': 13,\n",
       " 'couch': 5,\n",
       " 'concentrated': 11,\n",
       " 'bpa': 3,\n",
       " 'lgurowitz': 9,\n",
       " 'blender': 6,\n",
       " 'pouilly': 7,\n",
       " 'indicates': 8,\n",
       " 'jawbreaker': 9,\n",
       " '3to4': 4,\n",
       " 'liquore': 7,\n",
       " 'jalisco': 6,\n",
       " 'frappecrisco': 11,\n",
       " 'durack': 5,\n",
       " 'enough': 6,\n",
       " 'malouf': 4,\n",
       " 'topped': 6,\n",
       " 'humor': 4,\n",
       " '4x9': 3,\n",
       " 'others': 6,\n",
       " 'dhooghe': 7,\n",
       " 'cheesesteak': 10,\n",
       " 'igniting': 7,\n",
       " 'calgary': 6,\n",
       " 'yummmmy': 6,\n",
       " 'input': 5,\n",
       " 'messy': 4,\n",
       " 'countrylady': 10,\n",
       " 'child': 5,\n",
       " '311449': 6,\n",
       " 'meaning': 4,\n",
       " 'raters': 5,\n",
       " 'symon': 3,\n",
       " 'masse': 3,\n",
       " 'oomph': 4,\n",
       " 'thirst': 6,\n",
       " 'center': 5,\n",
       " 'alternative': 10,\n",
       " 'alsace': 5,\n",
       " 'paulas': 5,\n",
       " 'sambals': 5,\n",
       " 'dominant': 5,\n",
       " 'velveeta': 8,\n",
       " '482': 3,\n",
       " 'nipple': 6,\n",
       " 'gramme': 5,\n",
       " 'nutrious': 8,\n",
       " '1945': 4,\n",
       " 'dancer': 4,\n",
       " 'dehydration': 9,\n",
       " 'rubber': 6,\n",
       " 'ortiz': 5,\n",
       " 'vic': 3,\n",
       " 'carmel': 5,\n",
       " 'gainesville': 9,\n",
       " 'equivalent': 8,\n",
       " 'actually': 7,\n",
       " 'endangering': 9,\n",
       " 'gour': 4,\n",
       " 'tends': 4,\n",
       " 'carful': 5,\n",
       " 'improves': 7,\n",
       " 'submited': 7,\n",
       " 'defines': 6,\n",
       " 'cover': 5,\n",
       " 'pitting': 6,\n",
       " 'snack': 4,\n",
       " 'string': 5,\n",
       " 'fuggeddaboutit': 13,\n",
       " '102g': 4,\n",
       " 'wong': 3,\n",
       " 'misslemonie': 9,\n",
       " '375f': 4,\n",
       " 'demonstration': 10,\n",
       " 'release': 6,\n",
       " 'sautéing': 6,\n",
       " 'desert': 6,\n",
       " 'apologetic': 10,\n",
       " 'cursory': 7,\n",
       " 'ebook': 5,\n",
       " 'thickly': 7,\n",
       " 'chilindron': 9,\n",
       " 'halfway': 6,\n",
       " 'ommitted': 7,\n",
       " 'fritatta': 7,\n",
       " 'aux': 3,\n",
       " 'styled': 6,\n",
       " 'madaline': 5,\n",
       " 'auriga': 6,\n",
       " 'chocolaty': 8,\n",
       " 'wesolowski': 10,\n",
       " 'geaux': 4,\n",
       " 'slidell': 7,\n",
       " 'hebamare': 6,\n",
       " 'meantime': 5,\n",
       " 'letterboxing': 11,\n",
       " 'firming': 5,\n",
       " '409958': 6,\n",
       " 'spanish': 5,\n",
       " 'nanners': 5,\n",
       " 'accented': 7,\n",
       " 'men': 1,\n",
       " 'pesky': 5,\n",
       " 'heaven': 4,\n",
       " 'icecream': 7,\n",
       " 'cambridge': 8,\n",
       " 'applebee': 8,\n",
       " 'guasacaca': 8,\n",
       " 'browish': 7,\n",
       " 'madeleine': 6,\n",
       " 'marjoram': 6,\n",
       " 'giudia': 6,\n",
       " 'manged': 3,\n",
       " '8081': 4,\n",
       " 'repair': 5,\n",
       " 'sultry': 6,\n",
       " 'bobotie': 7,\n",
       " 'revising': 7,\n",
       " 'frosted': 7,\n",
       " 'broad': 4,\n",
       " 'flexible': 8,\n",
       " 'shook': 5,\n",
       " 'tb': 3,\n",
       " 'paraguay': 7,\n",
       " 'spice': 5,\n",
       " 'skilled': 7,\n",
       " 'omitting': 6,\n",
       " 'los': 3,\n",
       " 'slater': 5,\n",
       " 'serranos': 6,\n",
       " 'reconstitue': 10,\n",
       " 'morristown': 8,\n",
       " 'item': 4,\n",
       " 'dove': 4,\n",
       " 'varieities': 9,\n",
       " 'feverishly': 10,\n",
       " 'specially': 8,\n",
       " 'campus': 5,\n",
       " 'bulgar': 5,\n",
       " 'formidable': 8,\n",
       " 'bartlett': 7,\n",
       " 'brandy': 4,\n",
       " 'performance': 8,\n",
       " 'zarr': 3,\n",
       " 'disappointingly': 13,\n",
       " 'chris': 5,\n",
       " 'escoffier': 9,\n",
       " 'marmalade': 7,\n",
       " 'pointing': 7,\n",
       " 'pasticciata': 10,\n",
       " 'marramamba': 8,\n",
       " 'congregational': 12,\n",
       " 'advocaat': 7,\n",
       " 'glutenfreeinslc': 14,\n",
       " 'zmail': 3,\n",
       " 'obviously': 9,\n",
       " 'pollock': 7,\n",
       " '148': 3,\n",
       " 'ghanian': 5,\n",
       " 'senior': 5,\n",
       " 'moxie': 4,\n",
       " 'wakeman': 4,\n",
       " 'sooooooo': 8,\n",
       " 'corruption': 9,\n",
       " 'prewarn': 5,\n",
       " 'hoffmann': 5,\n",
       " 'go': 3,\n",
       " 'sonoma': 5,\n",
       " 'regarding': 7,\n",
       " 'gound': 4,\n",
       " 'baseball': 7,\n",
       " 'espress': 7,\n",
       " 'rolling': 6,\n",
       " 'cdkitchen': 8,\n",
       " 'saving': 4,\n",
       " 'overtone': 7,\n",
       " 'pesticide': 9,\n",
       " 'built': 5,\n",
       " 'farmer': 5,\n",
       " 'campanha': 5,\n",
       " 'absorbs': 7,\n",
       " '423482': 6,\n",
       " 'padma': 4,\n",
       " 'salumeria': 8,\n",
       " 'chocolatiest': 11,\n",
       " 'tatse': 4,\n",
       " 'fondue': 5,\n",
       " 'filter': 6,\n",
       " 'portale': 6,\n",
       " 'arrowroot': 9,\n",
       " 'prefered': 8,\n",
       " 'appropriate': 10,\n",
       " 'sweetcorn': 8,\n",
       " 'dorion': 5,\n",
       " 'enchilada': 8,\n",
       " 'cant': 2,\n",
       " 'dairy': 4,\n",
       " 'lite': 4,\n",
       " 'littleturtle': 12,\n",
       " 'embarrassed': 9,\n",
       " 'mccalls': 5,\n",
       " 'teenaged': 7,\n",
       " 'zum': 3,\n",
       " 'charring': 6,\n",
       " 'seiberts': 8,\n",
       " 'lecker': 6,\n",
       " 'italiano': 6,\n",
       " 'ticino': 5,\n",
       " 'kneaded': 6,\n",
       " 'steroid': 7,\n",
       " 'capri': 4,\n",
       " 'sequeira': 8,\n",
       " 'cole': 4,\n",
       " 'osso': 4,\n",
       " 'intimate': 6,\n",
       " 'affiliation': 9,\n",
       " 'bain': 2,\n",
       " 'islandteashop': 11,\n",
       " 'leanne': 4,\n",
       " 'card': 3,\n",
       " 'nneckredrecipes': 15,\n",
       " '40240': 5,\n",
       " 'adjustment': 8,\n",
       " 'guided': 6,\n",
       " 'costume': 7,\n",
       " 'mariner': 4,\n",
       " 'artisan': 5,\n",
       " 'booklet': 7,\n",
       " 'blah': 3,\n",
       " 'mansion': 4,\n",
       " ...}"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from Levenshtein import distance\n",
    "def k_closest(word, words, k):\n",
    "    return {w: distance(word, w) for w in words}\n",
    "k_closest('man', words, 5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Стемминг, лемматизация"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.1 На основе результатов 1.1 создайте `pd.DataFrame` со столбцами: \n",
    "    * word\n",
    "    * stemmed_word \n",
    "    * normalized_word \n",
    "\n",
    "Столбец `word` укажите в качестве индекса. \n",
    "\n",
    "Для стемминга воспользуйтесь `SnowballStemmer`, для нормализации слов - `WordNetLemmatizer`. Сравните результаты стемминга и лемматизации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          stemmed_word normalized_word\n",
      "word                                  \n",
      "digest          digest          digest\n",
      "denti            denti           denti\n",
      "spinning          spin        spinning\n",
      "volleball     vollebal       volleball\n",
      "hockett        hockett         hockett\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "data = []\n",
    "for word in words:\n",
    "    stemmed_word = stemmer.stem(word)\n",
    "    normalized_word = lemmatizer.lemmatize(word)\n",
    "    data.append((word, stemmed_word, normalized_word))\n",
    "df = pd.DataFrame(data, columns=['word', 'stemmed_word', 'normalized_word']).set_index('word')\n",
    "print(df.head())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2. Удалите стоп-слова из описаний рецептов. Какую долю об общего количества слов составляли стоп-слова? Сравните топ-10 самых часто употребляемых слов до и после удаления стоп-слов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'by', 'under', 'once', 'more', \"mightn't\", 'this', \"wouldn't\", 'all', \"wasn't\", 'with', 'do', 'd', 'yours', 'does', 'yourselves', 'shan', \"you've\", 'an', 'such', 'both', 'doing', 'own', 'he', \"doesn't\", 'its', \"needn't\", 'needn', 'mightn', 'wasn', 'wouldn', 'y', 'after', 'out', 'won', 'between', 'some', 'myself', 'being', 'isn', 'doesn', \"you'd\", 'them', \"don't\", 'your', 'we', 'yourself', 'you', 'did', 'their', 'hadn', 'they', 'should', 'me', \"won't\", 'above', 't', 'which', 'through', 'few', 'i', \"she's\", 'of', 'be', 'his', 'as', \"isn't\", \"weren't\", 'my', 'now', 'she', 'than', 'how', 'a', 'weren', 'are', 'and', 'but', 'to', \"should've\", 'no', 'ma', 'nor', 'was', 'when', 'o', 'or', \"aren't\", 'there', 'too', 'only', 'ain', 'where', 'other', 'himself', 'any', 'has', 'hers', 'having', 'have', 'then', 'very', 'on', 'couldn', 'these', 'am', 'in', 'below', \"it's\", 'what', 'her', 'up', 'from', 'hasn', 'here', 'most', 'whom', 'just', 'before', 'into', 'during', 'don', \"couldn't\", 'll', 'can', \"that'll\", 'haven', 'were', 'until', 'it', 'aren', 'those', 'each', 'itself', 'if', \"mustn't\", 'down', 'because', 'been', \"shouldn't\", 'the', \"hadn't\", 'over', 'against', 'herself', 'again', \"hasn't\", 'why', 'at', 're', \"haven't\", 'is', 'shouldn', 'so', 'off', 'about', 'ourselves', \"you're\", \"you'll\", 'him', 'that', 'for', 've', 'same', 's', \"shan't\", \"didn't\", 'while', 'm', 'will', 'ours', 'theirs', 'didn', 'not', 'had', 'further', 'mustn', 'who', 'our', 'themselves'}\n",
      "\n",
      " 0.0009486808818214673 \n",
      "\n",
      "digest         1\n",
      "reviver        1\n",
      "daytime        1\n",
      "cane           1\n",
      "elderflower    1\n",
      "pronouned      1\n",
      "wigal          1\n",
      "wrapper        1\n",
      "epicuirious    1\n",
      "waterway       1\n",
      "dtype: int64 \n",
      "\n",
      "\n",
      "\n",
      "digest         1\n",
      "reviver        1\n",
      "daytime        1\n",
      "cane           1\n",
      "elderflower    1\n",
      "pronouned      1\n",
      "wigal          1\n",
      "wrapper        1\n",
      "epicuirious    1\n",
      "waterway       1\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "words = list(words)\n",
    "stop_words = set(stopwords.words('english'))\n",
    "print(stop_words)\n",
    "filtered_words = [w for w in words if w.lower not in stop_words]\n",
    "top_10_1 = pd.Series(words).value_counts().head(10)\n",
    "top_10_2 = pd.Series(filtered_words).value_counts().head(10)\n",
    "stop_words_counter = len([w for w in words if w.lower() in stop_words])\n",
    "total_words = len(words)\n",
    "stop_words_c = stop_words_counter/total_words\n",
    "print('\\n',stop_words_c, '\\n')\n",
    "print(top_10_1, '\\n\\n\\n')\n",
    "print(top_10_2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Векторное представление текста"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.1 Выберите случайным образом 5 рецептов из набора данных. Представьте описание каждого рецепта в виде числового вектора при помощи `TfidfVectorizer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blue corn blueberry muffins: as with most muffins, these are great served warm with butter.\n",
      "sandy s holiday mashed potato casserole: my mother-in-law's fabulous holiday potato dish!  this should be made the day before eating.\n",
      "bleu cheese and pear salad: i got this salad from allrecipes to make for special dinner for my family coming over. it is fabulous! it so pretty and the flavors are so different but compliment each other so well. serve this to company or a special dinner and you won't be dissapointed. everyone will love it!\n",
      "mexican double chocolate chip cookies: this was posted in our local newspaper, the wenatchee world. a little change of pace and different from the every day chocolate chip cookie.\n",
      "spicy canned corn: i like spicy food..my boyfriend loves spicy food, so i've learned you can add a jalapeno or crushed red pepper to just about anything to give it that extra kick.  plus, the little green pieces really look nice with yellow corn.\n",
      "(5, 64)\n",
      "[[0.         0.         0.         0.4472136  0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.4472136  0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.4472136\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.4472136  0.         0.         0.         0.4472136\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.29857028 0.         0.         0.37007017 0.\n",
      "  0.37007017 0.         0.29857028 0.         0.         0.\n",
      "  0.         0.         0.         0.37007017 0.         0.\n",
      "  0.         0.37007017 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.37007017 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.37007017 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.20715955 0.         0.         0.         0.\n",
      "  0.         0.20715955 0.20715955 0.20715955 0.         0.\n",
      "  0.         0.         0.16713502 0.41431911 0.         0.20715955\n",
      "  0.         0.         0.16713502 0.20715955 0.20715955 0.\n",
      "  0.20715955 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.20715955 0.         0.20715955 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.20715955 0.         0.         0.20715955\n",
      "  0.20715955 0.         0.41431911 0.         0.         0.\n",
      "  0.         0.20715955 0.         0.        ]\n",
      " [0.         0.         0.         0.         0.28924517 0.28924517\n",
      "  0.28924517 0.         0.         0.         0.28924517 0.\n",
      "  0.         0.23336118 0.23336118 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.23336118 0.28924517\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.28924517 0.         0.28924517 0.         0.         0.\n",
      "  0.28924517 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.28924517 0.         0.28924517 0.        ]\n",
      " [0.18364578 0.         0.18364578 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.18364578\n",
      "  0.18364578 0.         0.         0.         0.         0.\n",
      "  0.         0.18364578 0.         0.         0.         0.36729155\n",
      "  0.         0.         0.18364578 0.         0.18364578 0.18364578\n",
      "  0.18364578 0.         0.18364578 0.18364578 0.14816426 0.\n",
      "  0.18364578 0.         0.18364578 0.         0.         0.\n",
      "  0.         0.18364578 0.         0.18364578 0.18364578 0.18364578\n",
      "  0.         0.         0.         0.18364578 0.18364578 0.\n",
      "  0.         0.         0.         0.36729155 0.18364578 0.\n",
      "  0.         0.         0.         0.18364578]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np \n",
    "recipes = pd.read_csv('recipes_sample.csv')\n",
    "sample_recipes = recipes.sample(n=5)\n",
    "sample_recipes.head()\n",
    "for i, row in sample_recipes.iterrows():\n",
    "    print(row['name'] + ': ' + row['description'])\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
    "tfidf_vectors = tfidf_vectorizer.fit_transform(sample_recipes['description'])\n",
    "print(tfidf_vectors.shape)\n",
    "print(tfidf_vectors.toarray())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.2 Вычислите близость между каждой парой рецептов, выбранных в задании 3.1, используя косинусное расстояние (`scipy.spatial.distance.cosine`) Результаты оформите в виде таблицы `pd.DataFrame`. В качестве названий строк и столбцов используйте названия рецептов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 3)\n",
      "(1, 3)\n",
      "0.007416666029069763\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial.distance import cosine\n",
    "a = np.array([[1, 2, 3]])\n",
    "b = np.array([[2, 3, 4]])\n",
    "print(a.shape)\n",
    "print(b.shape)\n",
    "a = a.ravel()\n",
    "b = b.ravel()\n",
    "distance = cosine(a, b)\n",
    "print(distance) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.3 Какие рецепты являются наиболее похожими? Прокомментируйте результат (словами)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "blue corn blueberry muffins, mashed potato casserole"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
